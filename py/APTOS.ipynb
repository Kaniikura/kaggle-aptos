{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hWBmhmCqCFpp"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import sys\n",
    "package_dir = \"../input/pretrained-models.pytorch-master/\"\n",
    "sys.path.insert(0, package_dir)\n",
    "import pretrainedmodels\n",
    "package_dir = '../input/early-stopping-pytorch'\n",
    "sys.path.append(package_dir)\n",
    "from pytorchtools import EarlyStopping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from functools import partial\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from collections import Counter\n",
    "import json\n",
    "import time\n",
    "import cv2\n",
    "import albumentations\n",
    "from albumentations import torch as AT\n",
    "import gc\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "#from PIL import Image, ImageFile\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import os\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "def quadratic_kappa(y_hat, y):\n",
    "    return torch.tensor(cohen_kappa_score(torch.round(y_hat), y, weights='quadratic'),device=device)\n",
    "#ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# To have reproducible results and compare them\n",
    "nr_seed = 2019\n",
    "import numpy as np \n",
    "np.random.seed(nr_seed)\n",
    "\n",
    "# Specify GPU usage\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5\"\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image1(img,tol=7):\n",
    "    # tol  is tolerance\n",
    "    mask = img>tol\n",
    "    return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "\n",
    "def crop_image_from_gray(img,tol=7):\n",
    "    if img.ndim ==2:\n",
    "        mask = img>tol\n",
    "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "    elif img.ndim==3:\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_img>tol\n",
    "        \n",
    "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
    "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
    "            return img \n",
    "        else:\n",
    "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img = np.stack([img1,img2,img3],axis=-1)\n",
    "        \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = albumentations.Compose([\n",
    "    albumentations.Resize(224, 224),\n",
    "    albumentations.HorizontalFlip(),\n",
    "    albumentations.RandomBrightness(),\n",
    "    albumentations.JpegCompression(80),\n",
    "    albumentations.HueSaturationValue(),\n",
    "    albumentations.Normalize(),\n",
    "    AT.ToTensor()\n",
    "    ])\n",
    "transform_valid_test = albumentations.Compose([\n",
    "    albumentations.Resize(224, 224),\n",
    "    albumentations.Normalize(),\n",
    "    AT.ToTensor()\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfのimage_idから対応する画像を読み込んだり、前処理をかけたりする関数\n",
    "class RetinopathyDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform, datatype='train'):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.datatype = datatype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.datatype=='train':\n",
    "            img_name = os.path.join('../input/aptos2019-blindness-detection/train_images',\n",
    "                                self.data.loc[idx, 'id_code'] + '.png')\n",
    "        elif self.datatype=='train_old':\n",
    "            img_name = os.path.join('../input/diabetic-retinopathy-resized/resized_train',\n",
    "                                self.data.loc[idx, 'image'] + '.jpeg')\n",
    "        else:\n",
    "            img_name =  os.path.join('../input/aptos2019-blindness-detection/test_images',\n",
    "                                     self.data.loc[idx, 'id_code'] + '.png')   \n",
    "        img = cv2.imread(img_name)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = self.circle_crop(img, sigmaX=10)\n",
    "        img = self.transform(image=img)\n",
    "        img = img['image']\n",
    "        if self.datatype=='train':\n",
    "            label = torch.tensor(self.data.loc[idx, 'diagnosis'])\n",
    "            return {'image': img,\n",
    "                    'labels': label\n",
    "                    }\n",
    "        elif self.datatype=='train_old':\n",
    "            label = torch.tensor(self.data.loc[idx, 'level'])\n",
    "            return {'image': img,\n",
    "                    'labels': label\n",
    "                    }\n",
    "        else:\n",
    "            return {'image': img}\n",
    "    def circle_crop(self, img, sigmaX=10):   \n",
    "        height, width, depth = img.shape    \n",
    "\n",
    "        x = int(width/2)\n",
    "        y = int(height/2)\n",
    "        r = np.amin((x,y))\n",
    "\n",
    "        circle_img = np.zeros((height, width), np.uint8)\n",
    "        cv2.circle(circle_img, (x,y), int(r), 1, thickness=-1)\n",
    "        img = cv2.bitwise_and(img, img, mask=circle_img)\n",
    "        img = crop_image_from_gray(img)\n",
    "        img = cv2.addWeighted ( img,4, cv2.GaussianBlur( img , (0,0) , sigmaX) ,-4 ,128)\n",
    "        return img \n",
    "    \n",
    "\n",
    "## kappa_lossにとる最適化をするclass。使い方勉強中\n",
    "class OptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "\n",
    "        ll = metrics.cohen_kappa_score(y, X_p, weights='quadratic')\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [0.5, 1.5, 2.5, 3.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "        return X_p\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tuvwjkp7K-oa"
   },
   "outputs": [],
   "source": [
    "def load_model(pretrained='imagenet'):\n",
    "    model = pretrainedmodels.__dict__['resnet101'](pretrained=pretrained)\n",
    "    model.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "    model.last_linear = nn.Sequential(\n",
    "                              nn.BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                              nn.Dropout(p=0.25),\n",
    "                              nn.Linear(in_features=2048, out_features=2048, bias=True),\n",
    "                              nn.ReLU(),\n",
    "                              nn.BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                              nn.Dropout(p=0.5),\n",
    "                              nn.Linear(in_features=2048, out_features=1, bias=True),\n",
    "                             )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train(object):\n",
    "    def __init__(self, params):\n",
    "        self.lr = params.get('lr')\n",
    "        self.img_size = params.get('img_size')\n",
    "        self.batch_size = params.get('batch_size')\n",
    "        self.n_epochs = params.get('n_epochs')\n",
    "        self.n_freeze = params.get('n_freeze')\n",
    "        self.coef = params.get('coef')\n",
    "        self.criterion = params.get('criterion')\n",
    "        self.num_workers = params.get('num_workers')\n",
    "        self.load_state = params.get('load_state')\n",
    "        self.load_path = params.get('load_path')\n",
    "        self.save_path = params.get('save_path')\n",
    "        self.device = params.get('device')\n",
    "        self.n_folds = params.get('n_folds')\n",
    "        self.use_valid = False\n",
    "        self.early_stop = params.get(\"early_stop\"),\n",
    "        self.patience = params.get(\"patience\"),\n",
    "    \n",
    "    def get_train(self,data):\n",
    "        self.train_data = data\n",
    "        \n",
    "    def get_valid(self,data):\n",
    "        self.valid_data = data\n",
    "        self.use_valid = True\n",
    "        \n",
    "    def fit(self, use_cv=False, train_idx=None, valid_idx=None):\n",
    "        since = time.time()\n",
    "        # Model\n",
    "        if self.load_state:\n",
    "            model = load_model(pretrained=None)\n",
    "            model.load_state_dict(torch.load(self.load_path, map_location=self.device))\n",
    "        else:\n",
    "            model = load_model(pretrained='imagenet')\n",
    "        model = model.to(device)\n",
    "\n",
    "        \n",
    "        plist = [{'params': model.parameters(), 'lr': self.lr}]\n",
    "        optimizer = optim.Adam(plist, lr=self.lr)\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10)\n",
    "        \n",
    "        if use_cv:\n",
    "            train_sampler = SubsetRandomSampler(train_idx)\n",
    "            valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "            data_loader_train = torch.utils.data.DataLoader(self.train_data, batch_size=self.batch_size,\n",
    "                                                num_workers=self.num_workers,sampler=train_sampler)\n",
    "            data_loader_valid = torch.utils.data.DataLoader(self.train_data, batch_size=self.batch_size,\n",
    "                                                num_workers=self.num_workers,sampler=valid_sampler)\n",
    "        elif self.use_valid:\n",
    "            print(\"OK\")\n",
    "            data_loader_train = torch.utils.data.DataLoader(self.train_data, batch_size=self.batch_size,\n",
    "                                                num_workers=self.num_workers)\n",
    "            data_loader_valid = torch.utils.data.DataLoader(self.valid_data, batch_size=self.batch_size,\n",
    "                                                num_workers=self.num_workers)\n",
    "        else:\n",
    "            data_loader_train = torch.utils.data.DataLoader(self.train_data, batch_size=self.batch_size,\n",
    "                                                num_workers=self.num_workers)\n",
    "            \n",
    "        if self.early_stop:\n",
    "            early_stopping = EarlyStopping(patience=self.patience, verbose=True)\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            if epoch == self.n_freeze:      \n",
    "                for param in model.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "            print('Epoch {}/{}'.format(epoch, self.n_epochs - 1))\n",
    "            print('-' * 10)\n",
    "            scheduler.step()\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            tk0 = tqdm(data_loader_train, total=int(len(data_loader_train)))\n",
    "            counter = 0\n",
    "            for bi, d in enumerate(tk0):\n",
    "                inputs = d[\"image\"]\n",
    "                labels = d[\"labels\"].view(-1, 1)\n",
    "                inputs = inputs.to(self.device, dtype=torch.float)\n",
    "                labels = labels.to(self.device, dtype=torch.float)\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(True):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                counter += 1\n",
    "                tk0.set_postfix(loss=(running_loss / (counter * data_loader_train.batch_size)))\n",
    "            epoch_loss = running_loss / len(data_loader_train)\n",
    "            print('Training Loss: {:.4f}'.format(epoch_loss))\n",
    "            \n",
    "            if self.use_valid:\n",
    "                model.eval()\n",
    "                eval_loss = 0\n",
    "                nb_eval_steps = 0\n",
    "\n",
    "                for step, batch in enumerate(tqdm(data_loader_valid , total=int(len(data_loader_valid)))):\n",
    "\n",
    "                    inputs = batch[\"image\"]\n",
    "                    labels = batch[\"labels\"].view(-1, 1)\n",
    "\n",
    "                    inputs = inputs.to(self.device, dtype=torch.float)\n",
    "                    labels = labels.to(self.device, dtype=torch.float)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(inputs)\n",
    "\n",
    "                    y_hat = torch.Tensor.cpu(outputs.view(-1))\n",
    "                    y = torch.Tensor.cpu(labels.view(-1))\n",
    "\n",
    "                    for pred in enumerate(y_hat):\n",
    "                        if pred[1] < self.coef[0]:\n",
    "                            y_hat[1] = 0\n",
    "                        elif pred[1] >= self.coef[0] and pred[1] < self.coef[1]:\n",
    "                            y_hat[1] = 1\n",
    "                        elif pred[1] >= self.coef[1] and pred[1] < self.coef[2]:\n",
    "                            y_hat[1] = 2\n",
    "                        elif pred[1] >= self.coef[2] and pred[1] < self.coef[3]:\n",
    "                            y_hat[1] = 3\n",
    "                        else:\n",
    "                            y_hat[1] = 4\n",
    "\n",
    "                    tmp_eval_loss = quadratic_kappa(y_hat, y)\n",
    "\n",
    "                    eval_loss += tmp_eval_loss.mean().item()\n",
    "                    nb_eval_steps += 1\n",
    "\n",
    "                eval_loss = eval_loss / nb_eval_steps\n",
    "\n",
    "                print('Validation Kappa: {:.4f}'.format(eval_loss))\n",
    "\n",
    "                if self.early_stop:\n",
    "                    eval_loss = 1 - eval_loss\n",
    "                    early_stopping(eval_loss, model)\n",
    "                    if early_stopping.early_stop:\n",
    "                        print(\"Early stopping\")\n",
    "                        break\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "        \n",
    "        if not use_cv:\n",
    "            torch.save(model.state_dict(), self.save_path)\n",
    "\n",
    "        del(model, data_loader_train)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() \n",
    "        \n",
    "    def cv(self):\n",
    "        num_train = len(self.train_data)\n",
    "        indices = list(range(num_train))\n",
    "        kf = KFold(n_splits=self.n_folds, random_state=1337, shuffle=True)\n",
    "\n",
    "        train_idx = []\n",
    "        valid_idx = []\n",
    "\n",
    "        for t, v in kf.split(indices):\n",
    "            train_idx.append(t)\n",
    "            valid_idx.append(v)\n",
    "\n",
    "        # Training                        \n",
    "        for fold in np.arange(self.n_folds):\n",
    "            print('Fold:',fold)\n",
    "            self.fit(use_cv=True,train_idx=train_idx[fold],valid_idx=valid_idx[fold])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params\n",
    "params = {\n",
    "    \"lr\" : 1e-5,\n",
    "    \"img_size\" : 224,\n",
    "    \"batch_size\" : 48,\n",
    "    \"n_epochs\" : 1,\n",
    "    \"n_freeze\" : 1,\n",
    "    \"num_workers\" : 8,\n",
    "    \"coef\" : [0.5, 1.5, 2.5, 3.5],\n",
    "    \"criterion\" : nn.MSELoss(),\n",
    "    \"load_state\" : False,# fine-tuning済みのモデルを使うか\n",
    "    \"load_path\" : None, \n",
    "    'save_path' : None,\n",
    "    \"device\" : device,\n",
    "    \"n_folds\" : 2,\n",
    "    \"early_stop\" : True,\n",
    "    \"patience\" : 2,\n",
    "}\n",
    "\n",
    "#training for the lazy, like me\n",
    "\n",
    "# cv: new\n",
    "if 0:\n",
    "    Mytrain = train(params)\n",
    "    Mytrain.get_train(RetinopathyDataset(csv_file=\"../input/aptos2019-blindness-detection/train.csv\",\n",
    "                                            transform=transform, datatype='train'))\n",
    "    Mytrain.cv()\n",
    "\n",
    "# train:old & valid:new\n",
    "elif 0:\n",
    "    \n",
    "    params['save_path'] =  \"../input/mmmodel/model_old_test.bin\"\n",
    "    Mytrain = train(params)\n",
    "    Mytrain.get_train(RetinopathyDataset(csv_file=\"../input/diabetic-retinopathy-resized/trainLabels.csv\",\n",
    "                                            transform=transform, datatype='train_old'))\n",
    "    Mytrain.get_valid(RetinopathyDataset(csv_file=\"../input/aptos2019-blindness-detection/train.csv\",\n",
    "                                           transform=transform, datatype='train'))\n",
    "    Mytrain.fit()\n",
    "    \n",
    "# train:new\n",
    "elif 1:\n",
    "    params['n_epochs'] = 1\n",
    "    params['load_state'] = True\n",
    "    params['load_path'] =  \"../input/mmmodel/model_old_test.bin\" \n",
    "    params['save_path'] =  \"../input/mmmodel/model_new_test.bin\"        \n",
    "    Mytrain = train(params)\n",
    "    Mytrain.get_train(RetinopathyDataset(csv_file=\"../input/aptos2019-blindness-detection/train.csv\",\n",
    "                                           transform=transform, datatype='train'))\n",
    "    Mytrain.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(pretrained=None)\n",
    "model.load_state_dict(torch.load(\"../input/mmmodel/model_0726_01.bin\", map_location=device))\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = RetinopathyDataset(csv_file='../input/aptos2019-blindness-detection/sample_submission.csv',\n",
    "                                      transform=transform_valid_test, datatype='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bM6q_rVj2hTR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_bs = 32\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_bs, shuffle=False, num_workers=4)\n",
    "test_preds = np.zeros((len(test_dataset), 1))\n",
    "tk0 = tqdm(test_data_loader)\n",
    "for i, x_batch in tqdm(enumerate(tk0)):\n",
    "    x_batch = x_batch[\"image\"]\n",
    "    pred = model(x_batch.to(device))\n",
    "    test_preds[i * test_bs:(i + 1) * test_bs] = pred.detach().cpu().squeeze().numpy().ravel().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bbGSxlHC2jpq"
   },
   "outputs": [],
   "source": [
    "coef = [0.5, 1.5, 2.5, 3.5]\n",
    "\n",
    "for i, pred in enumerate(test_preds):\n",
    "    if pred < coef[0]:\n",
    "        test_preds[i] = 0\n",
    "    elif pred >= coef[0] and pred < coef[1]:\n",
    "        test_preds[i] = 1\n",
    "    elif pred >= coef[1] and pred < coef[2]:\n",
    "        test_preds[i] = 2\n",
    "    elif pred >= coef[2] and pred < coef[3]:\n",
    "        test_preds[i] = 3\n",
    "    else:\n",
    "        test_preds[i] = 4\n",
    "\n",
    "\n",
    "sample = pd.read_csv(\"../input/aptos2019-blindness-detection/sample_submission.csv\")\n",
    "sample.diagnosis = test_preds.astype(int)\n",
    "sample.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vBIamI-j2nDC"
   },
   "outputs": [],
   "source": [
    "sample[\"diagnosis\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "APTOS.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
