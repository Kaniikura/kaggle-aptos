{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hWBmhmCqCFpp"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import sys\n",
    "package_dir = '../input/early-stopping-pytorch'\n",
    "sys.path.append(package_dir)\n",
    "from pytorchtools import EarlyStopping\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "from collections import Counter, OrderedDict\n",
    "from itertools import chain\n",
    "import json\n",
    "import math\n",
    "import numbers\n",
    "import time\n",
    "import cv2\n",
    "import gc\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import albumentations\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pynverse import inversefunc\n",
    "\n",
    "IMG_SIZE = 256\n",
    "\n",
    "# To have reproducible results and compare them\n",
    "seedValue = 2019\n",
    "np.random.seed(seedValue)\n",
    "torch.manual_seed(seedValue)\n",
    "os.environ['PYTHONHASHSEED'] = str(seedValue)\n",
    "torch.cuda.manual_seed(seedValue)\n",
    "torch.cuda.manual_seed_all(seedValue) \n",
    "# 学習速度が遅くなるらしく、以下２行はコメントアウト\n",
    "#torch.backends.cudnn.deterministic = True  \n",
    "#torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Specify GPU usage\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,7,8,9\" \n",
    "device_ids = [0,1,2,3]\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "def quadratic_kappa(y_hat, y, coef):\n",
    "    for pred in enumerate(y_hat):\n",
    "            if pred[1] < coef[0]:\n",
    "                y_hat[1] = 0\n",
    "            elif pred[1] >= coef[0] and pred[1] < coef[1]:\n",
    "                y_hat[1] = 1\n",
    "            elif pred[1] >= coef[1] and pred[1] < coef[2]:\n",
    "                y_hat[1] = 2\n",
    "            elif pred[1] >= coef[2] and pred[1] < coef[3]:\n",
    "                y_hat[1] = 3\n",
    "            else:\n",
    "                y_hat[1] = 4\n",
    "                \n",
    "    return torch.tensor(cohen_kappa_score(torch.round(y_hat), y, weights='quadratic'),device=device)\n",
    "\n",
    "# confusion matrix\n",
    "def plot_cmx(true, output):\n",
    "    labels = [0,1,2,3,4]\n",
    "    cmx = confusion_matrix(true, output,labels=labels)\n",
    "    plt.figure(figsize=(6,4)) \n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    sns.heatmap(cmx, annot = True)\n",
    "    plt.show()\n",
    "    \n",
    "# calculate scale from ratio\n",
    "inv = inversefunc(lambda x : (np.arcsin(2*x**2-1)+2*x*np.sqrt(1-x**2))/(2*x**2),domain=[0.7,1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image1(img,tol=7):\n",
    "    # 'tol' is tolerance\n",
    "    mask = img>tol\n",
    "    return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "\n",
    "def crop_image_from_gray(img,tol=7):\n",
    "    if img.ndim ==2:\n",
    "        mask = img>tol\n",
    "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "    elif img.ndim==3:\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_img>tol\n",
    "        \n",
    "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
    "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
    "            return img \n",
    "        else:\n",
    "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img = np.stack([img1,img2,img3],axis=-1)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "\n",
    "def zoom_to_center(img, tol=7, th = 0.95 ,p= 1.0):\n",
    "    img = crop_image_from_gray(img, tol = tol) \n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    s1 = gray_img.size\n",
    "    mask = gray_img > 7\n",
    "    s2 = mask.sum()\n",
    "    # calculate Ratio of object to image\n",
    "    ratio = s2/s1\n",
    "    if ratio <= th:\n",
    "        # The larger rest you have, the more a image is zoomed.\n",
    "        rest = np.sqrt(2)*inv(ratio)-1 if (ratio>= np.pi/4) else np.sqrt(2)-1\n",
    "        sl = max(rest-0.125,0)\n",
    "        aug = albumentations.ShiftScaleRotate(shift_limit = 0.01, scale_limit=(sl,sl + 0.05),\n",
    "                                              rotate_limit=5, p=p)\n",
    "        img = aug(image=img)['image']\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinopathyDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform, datatype='train'):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.datatype = datatype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 画像データの読み込み\n",
    "        if self.datatype=='train':\n",
    "            img_name = os.path.join('../input/aptos2019-blindness-detection/train_images',\n",
    "                                self.data.loc[idx, 'id_code'] + '.png')\n",
    "            label = self.data.loc[idx, 'diagnosis']\n",
    "        elif self.datatype=='train_old':\n",
    "            img_name = os.path.join('../input/diabetic-retinopathy-resized/resized_train',\n",
    "                                self.data.loc[idx, 'image'] + '.jpeg')\n",
    "            label = self.data.loc[idx, 'level']\n",
    "        else:\n",
    "            img_name =  os.path.join('../input/aptos2019-blindness-detection/test_images',\n",
    "                                     self.data.loc[idx, 'id_code'] + '.png')\n",
    "        # image preprocessing\n",
    "        img = cv2.imread(img_name)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = zoom_to_center(img)\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        img = self.transform(image=img)['image']\n",
    "        img = torch.from_numpy(img).permute(2,0,1)\n",
    "       \n",
    "        if self.datatype=='train':\n",
    "            return {'image': img,\n",
    "                    'labels': label\n",
    "                    }\n",
    "        elif self.datatype=='train_old':\n",
    "            return {'image': img,\n",
    "                    'labels': label\n",
    "                    }\n",
    "        else:\n",
    "            return {'image': img}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "transform = albumentations.Compose([\n",
    "    albumentations.HorizontalFlip(),\n",
    "    albumentations.VerticalFlip(),\n",
    "    albumentations.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "tuvwjkp7K-oa"
   },
   "outputs": [],
   "source": [
    "def RMSELoss(yhat,y):\n",
    "    return torch.sqrt(torch.mean((yhat-y)**2))\n",
    "    \n",
    "class AdaptiveConcatPool2d(nn.Module):\n",
    "    def __init__(self,sz=1):\n",
    "        super().__init__()\n",
    "        self.output_size = sz\n",
    "        self.ap = nn.AdaptiveAvgPool2d(sz)\n",
    "        self.mp = nn.AdaptiveMaxPool2d(sz)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return torch.cat([self.mp(x),self.ap(x)],1)\n",
    "    \n",
    "class RAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.buffer = [[None, None, None] for ind in range(10)]\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = self.buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:            \n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-step_size, exp_avg)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.reshape(x.shape[0], -1)\n",
    "    \n",
    "class SqueezeExcitation(nn.Module):\n",
    "    \n",
    "    def __init__(self, inplanes, se_planes):\n",
    "        super(SqueezeExcitation, self).__init__()\n",
    "        self.reduce_expand = nn.Sequential(\n",
    "            nn.Conv2d(inplanes, se_planes, \n",
    "                      kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            Swish(),\n",
    "            nn.Conv2d(se_planes, inplanes, \n",
    "                      kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_se = torch.mean(x, dim=(-2, -1), keepdim=True)\n",
    "        x_se = self.reduce_expand(x_se)\n",
    "        return x_se * x\n",
    "    \n",
    "class MBConv(nn.Module):\n",
    "    def __init__(self, inplanes, planes, kernel_size, stride, \n",
    "                 expand_rate=1.0, se_rate=0.25, \n",
    "                 drop_connect_rate=0.2):\n",
    "        super(MBConv, self).__init__()\n",
    "\n",
    "        expand_planes = int(inplanes * expand_rate)\n",
    "        se_planes = max(1, int(inplanes * se_rate))\n",
    "\n",
    "        self.expansion_conv = None        \n",
    "        if expand_rate > 1.0:\n",
    "            self.expansion_conv = nn.Sequential(\n",
    "                nn.Conv2d(inplanes, expand_planes, \n",
    "                          kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(expand_planes, momentum=0.01, eps=1e-3),\n",
    "                Swish()\n",
    "            )\n",
    "            inplanes = expand_planes\n",
    "\n",
    "        self.depthwise_conv = nn.Sequential(\n",
    "            nn.Conv2d(inplanes, expand_planes,\n",
    "                      kernel_size=kernel_size, stride=stride, \n",
    "                      padding=kernel_size // 2, groups=expand_planes,\n",
    "                      bias=False),\n",
    "            nn.BatchNorm2d(expand_planes, momentum=0.01, eps=1e-3),\n",
    "            Swish()\n",
    "        )\n",
    "\n",
    "        self.squeeze_excitation = SqueezeExcitation(expand_planes, se_planes)\n",
    "        \n",
    "        self.project_conv = nn.Sequential(\n",
    "            nn.Conv2d(expand_planes, planes, \n",
    "                      kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(planes, momentum=0.01, eps=1e-3),\n",
    "        )\n",
    "\n",
    "        self.with_skip = stride == 1\n",
    "        self.drop_connect_rate = torch.tensor(drop_connect_rate, requires_grad=False)\n",
    "    \n",
    "    def _drop_connect(self, x):        \n",
    "        keep_prob = 1.0 - self.drop_connect_rate\n",
    "        drop_mask = torch.rand(x.shape[0], 1, 1, 1) + keep_prob\n",
    "        drop_mask = drop_mask.type_as(x)\n",
    "        drop_mask.floor_()\n",
    "        return drop_mask * x / keep_prob\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = x\n",
    "        if self.expansion_conv is not None:\n",
    "            x = self.expansion_conv(x)\n",
    "\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = self.squeeze_excitation(x)\n",
    "        x = self.project_conv(x)\n",
    "        \n",
    "        # Add identity skip\n",
    "        if x.shape == z.shape and self.with_skip:            \n",
    "            if self.training and self.drop_connect_rate is not None:\n",
    "                self._drop_connect(x)\n",
    "            x += z\n",
    "        return x\n",
    "    \n",
    "def init_weights(module):    \n",
    "    if isinstance(module, nn.Conv2d):    \n",
    "        nn.init.kaiming_normal_(module.weight, a=0, mode='fan_out')\n",
    "    elif isinstance(module, nn.Linear):\n",
    "        init_range = 1.0 / math.sqrt(module.weight.shape[1])\n",
    "        nn.init.uniform_(module.weight, a=-init_range, b=init_range)\n",
    "        \n",
    "class EfficientNet(nn.Module):\n",
    "        \n",
    "    def _setup_repeats(self, num_repeats):\n",
    "        return int(math.ceil(self.depth_coefficient * num_repeats))\n",
    "    \n",
    "    def _setup_channels(self, num_channels):\n",
    "        num_channels *= self.width_coefficient\n",
    "        new_num_channels = math.floor(num_channels / self.divisor + 0.5) * self.divisor\n",
    "        new_num_channels = max(self.divisor, new_num_channels)\n",
    "        if new_num_channels < 0.9 * num_channels:\n",
    "            new_num_channels += self.divisor\n",
    "        return new_num_channels\n",
    "\n",
    "    def __init__(self, num_classes, \n",
    "                 width_coefficient=1.0,\n",
    "                 depth_coefficient=1.0,\n",
    "                 se_rate=0.25,\n",
    "                 dropout_rate=0.2,\n",
    "                 drop_connect_rate=0.2):\n",
    "        super(EfficientNet, self).__init__()\n",
    "        \n",
    "        self.width_coefficient = width_coefficient\n",
    "        self.depth_coefficient = depth_coefficient\n",
    "        self.divisor = 8\n",
    "                \n",
    "        list_channels = [32, 16, 24, 40, 80, 112, 192, 320, 1280]\n",
    "        list_channels = [self._setup_channels(c) for c in list_channels]\n",
    "                \n",
    "        list_num_repeats = [1, 2, 2, 3, 3, 4, 1]\n",
    "        list_num_repeats = [self._setup_repeats(r) for r in list_num_repeats]        \n",
    "        \n",
    "        expand_rates = [1, 6, 6, 6, 6, 6, 6]\n",
    "        strides = [1, 2, 2, 2, 1, 2, 1]\n",
    "        kernel_sizes = [3, 3, 5, 3, 5, 5, 3]\n",
    "\n",
    "        # Define stem:\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, list_channels[0], kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(list_channels[0], momentum=0.01, eps=1e-3),\n",
    "            Swish()\n",
    "        )\n",
    "        \n",
    "        # Define MBConv blocks\n",
    "        blocks = []\n",
    "        counter = 0\n",
    "        num_blocks = sum(list_num_repeats)\n",
    "        for idx in range(7):\n",
    "            \n",
    "            num_channels = list_channels[idx]\n",
    "            next_num_channels = list_channels[idx + 1]\n",
    "            num_repeats = list_num_repeats[idx]\n",
    "            expand_rate = expand_rates[idx]\n",
    "            kernel_size = kernel_sizes[idx]\n",
    "            stride = strides[idx]\n",
    "            drop_rate = drop_connect_rate * counter / num_blocks\n",
    "            \n",
    "            name = \"MBConv{}_{}\".format(expand_rate, counter)\n",
    "            blocks.append((\n",
    "                name,\n",
    "                MBConv(num_channels, next_num_channels, \n",
    "                       kernel_size=kernel_size, stride=stride, expand_rate=expand_rate, \n",
    "                       se_rate=se_rate, drop_connect_rate=drop_rate)\n",
    "            ))\n",
    "            counter += 1\n",
    "            for i in range(1, num_repeats):                \n",
    "                name = \"MBConv{}_{}\".format(expand_rate, counter)\n",
    "                drop_rate = drop_connect_rate * counter / num_blocks                \n",
    "                blocks.append((\n",
    "                    name,\n",
    "                    MBConv(next_num_channels, next_num_channels, \n",
    "                           kernel_size=kernel_size, stride=1, expand_rate=expand_rate, \n",
    "                           se_rate=se_rate, drop_connect_rate=drop_rate)                                    \n",
    "                ))\n",
    "                counter += 1\n",
    "        \n",
    "        self.blocks = nn.Sequential(OrderedDict(blocks))\n",
    "        \n",
    "        # Define head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(list_channels[-2], list_channels[-1], \n",
    "                      kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(list_channels[-1], momentum=0.01, eps=1e-3),\n",
    "            Swish(),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            Flatten(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(list_channels[-1], num_classes)\n",
    "        )\n",
    "\n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        f = self.stem(x)\n",
    "        f = self.blocks(f)\n",
    "        y = self.head(f)\n",
    "        return y\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_EfficientNet():\n",
    "    model = EfficientNet(num_classes=1000, \n",
    "                     width_coefficient=1.4, depth_coefficient=1.8,\n",
    "                     dropout_rate=0.4)\n",
    "\n",
    "    model_state = torch.load(\"../data/model/efficientnet-pytorch/efficientnet-b4-e116e8b3.pth\")\n",
    "\n",
    "    # A basic remapping is required\n",
    "    mapping = {\n",
    "        k: v for k, v in zip(model_state.keys(), model.state_dict().keys())\n",
    "    }\n",
    "    mapped_model_state = OrderedDict([\n",
    "        (mapping[k], v) for k, v in model_state.items()\n",
    "    ])\n",
    "\n",
    "    model.load_state_dict(mapped_model_state, strict=False)\n",
    "    \n",
    "    \n",
    "    model.head[6] = nn.Sequential(\n",
    "                                  nn.Linear(in_features=1792, out_features=1024, bias=True),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                                  nn.Dropout(p=0.5),\n",
    "                                  nn.Linear(in_features=1024, out_features=6, bias=True),\n",
    "                                 ) # classification +  kappa regressor\n",
    "    \n",
    "    # freeze\n",
    "    for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    # unfreeze head\n",
    "    for param in model.head[6].parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "df_train = pd.read_csv('../input/aptos2019-blindness-detection/train.csv')\n",
    "df_test = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')\n",
    "\n",
    "x = df_train['id_code']\n",
    "y = df_train['diagnosis']\n",
    "\n",
    "x, y = shuffle(x, y)\n",
    "_ = y.hist()\n",
    "\n",
    "# get class stats\n",
    "n_classes = int(y.max()+1)\n",
    "class_weights = len(y) / df_train.groupby('diagnosis').count().values.ravel()  # we can use this to balance our loss function\n",
    "class_weights *= n_classes / class_weights.sum()\n",
    "print('class_weights:', class_weights.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.utils as vutils\n",
    "\n",
    "PLOT = False\n",
    "if PLOT:\n",
    "    data_dic = {\n",
    "                'train':{'csv':'../input/aptos2019-blindness-detection/train.csv','datatype':'train'},\n",
    "                'test':{'csv':'../input/aptos2019-blindness-detection/test.csv','datatype':'test'},\n",
    "                '2015':{'csv':'../input/diabetic-retinopathy-resized/new_trainLabels.csv',\n",
    "                        'datatype':'train_old'},      \n",
    "               }\n",
    "    select = '2015'\n",
    "    sample_dataset = RetinopathyDataset(csv_file=data_dic[select]['csv'],\n",
    "                                    transform = transform,\n",
    "                                    datatype=data_dic[select]['datatype'])\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(sample_dataset, batch_size=64,\n",
    "                                         num_workers=8, drop_last=True)\n",
    "    batch = next(iter(loader))\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Training Images\")\n",
    "    _ = plt.imshow( \n",
    "        vutils.make_grid(batch['image'][:16], padding=2, normalize=True).cpu().numpy().transpose((1, 2, 0))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation = lambda y: (n_classes-1) * torch.sigmoid(y)\n",
    "# activation = lambda y: (n_classes-1) * (0.5 + 0.5 * y / (1 + y.abs()))  # linear sigmoid\n",
    "activation = lambda y: y  # no-op\n",
    "\n",
    "def cont_kappa(input, targets, activation=None):\n",
    "    ''' continuos version of quadratic weighted kappa '''\n",
    "    n = len(targets)\n",
    "    y = targets.float().unsqueeze(0)\n",
    "    pred = input.float().squeeze(-1).unsqueeze(0)\n",
    "    if activation is not None:\n",
    "        pred = activation(pred)\n",
    "    wo = (pred - y)**2\n",
    "    we = (pred - y.t())**2\n",
    "    return 1 - (n * wo.sum() / we.sum())\n",
    "# adapted from keras version: https://www.kaggle.com/ryomiyazaki/keras-simple-implementation-of-qwk-for-regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_loss = lambda pred, y: 1 - cont_kappa(pred, y)  # from 0 to 2 instead of 1 to -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_one_hot(true_labels: torch.Tensor, classes = 5, smoothing=0.1):\n",
    "    \"\"\"\n",
    "    if smoothing == 0, it's one-hot method\n",
    "    if 0 < smoothing < 1, it's smooth method\n",
    "\n",
    "    \"\"\"\n",
    "    assert 0 <= smoothing < 1\n",
    "    confidence = 1.0 - smoothing\n",
    "    label_shape = torch.Size((true_labels.size(0), classes))\n",
    "    with torch.no_grad():\n",
    "        true_dist = torch.empty(size=label_shape, device=true_labels.device)\n",
    "        true_dist.fill_(smoothing / (classes - 1))\n",
    "        true_dist.scatter_(1, true_labels.data.unsqueeze(1), confidence)\n",
    "    return true_dist\n",
    "\n",
    "\n",
    "pattern =torch.FloatTensor([\n",
    "        [0.7, 0.15 ,0.075, 0.05, 0.025],\n",
    "        [0.1125, 0.7, 0.1125, 0.05, 0.025],\n",
    "        [0.025, 0.125, 0.7, 0.125, 0.025],\n",
    "        [0.025, 0.05, 0.1125, 0.7, 0.1125],\n",
    "        [0.025, 0.05, 0.075, 0.15, 0.7]\n",
    "         ])\n",
    "\n",
    "def smooth_pattern(true_labels, pattern=pattern):\n",
    "    label_shape = torch.Size((true_labels.size(0), 5))\n",
    "    pattern = pattern.to(true_labels.device)\n",
    "    with torch.no_grad():\n",
    "        true_dist = torch.zeros(size=label_shape, device=true_labels.device)\n",
    "        true_dist = true_dist.scatter_(1, true_labels.data.unsqueeze(1), 1)\n",
    "        true_dist = torch.einsum('bi,ik->bk', true_dist, pattern)\n",
    "    return true_dist\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2., reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = torch.tensor(alpha)\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        self.alpha = self.alpha.type(inputs.type(), non_blocking=True) # fix type and device\n",
    "        smooth_targets = smooth_pattern(targets)\n",
    "        CE_loss = nn.BCEWithLogitsLoss()(inputs, smooth_targets)\n",
    "        pt = torch.exp(-CE_loss)\n",
    "        F_loss = self.alpha[targets] * (1-pt)**self.gamma * CE_loss\n",
    "\n",
    "        if self.reduction == 'sum':\n",
    "            return F_loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            return F_loss.mean()\n",
    "        return F_loss\n",
    "    \n",
    "# balance between metric optimisation and classification accuracy\n",
    "class MultiTaskLoss(FocalLoss):\n",
    "    def __init__(self, alpha=None, gamma=2.0, second_loss=F.mse_loss, second_mult=0.1):\n",
    "        super().__init__(alpha, gamma)\n",
    "        self.second_loss = second_loss\n",
    "        self.second_mult = second_mult\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        loss  = super().forward(inputs[...,:-1], targets)  # focal loss\n",
    "        loss += self.second_mult * self.second_loss(inputs[...,-1], targets.float())\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train(object):\n",
    "    def __init__(self, params):\n",
    "        # params\n",
    "        self.lr = params.get('lr')\n",
    "        self.bs = params.get('batch_size')\n",
    "        self.n_epochs = params.get('n_epochs')\n",
    "        self.n_freeze = params.get('n_freeze')\n",
    "        self.coef = params.get('coef')\n",
    "        self.criterion = params.get('criterion')\n",
    "        self.num_workers = params.get('num_workers')\n",
    "        self.load_state = params.get('load_state')\n",
    "        self.load_path = params.get('load_path')\n",
    "        self.save_path = params.get('save_path')\n",
    "        self.device = params.get('device')\n",
    "        self.n_folds = params.get('n_folds')\n",
    "        self.use_valid = False\n",
    "        self.early_stop = params.get(\"early_stop\")\n",
    "        self.patience = params.get(\"patience\")\n",
    "        self.T_max = params.get(\"T_max\")\n",
    "    \n",
    "    def get_train(self,data):\n",
    "        self.train_data = data\n",
    "        \n",
    "    def get_valid(self,data):\n",
    "        self.valid_data = data\n",
    "        self.use_valid = True\n",
    "    \n",
    "    def fit(self, use_cv=False, train_idx=None, valid_idx=None):\n",
    "        since = time.time()\n",
    "        # Model\n",
    "        model = load_EfficientNet()\n",
    "        if self.load_state:\n",
    "            # load params\n",
    "            model_state = torch.load(self.load_path)\n",
    "            # A basic remapping is required\n",
    "            mapping = {\n",
    "                k: v for k, v in zip(model_state.keys(), model.state_dict().keys())\n",
    "            }\n",
    "            mapped_model_state = OrderedDict([\n",
    "                (mapping[k], v) for k, v in model_state.items()\n",
    "            ])\n",
    "            model.load_state_dict(mapped_model_state, strict=False)\n",
    "\n",
    "        if torch.cuda.device_count() > 1: # train in parallel if multi GPUs available\n",
    "            model = nn.DataParallel(model, device_ids)\n",
    "        model = model.to(device)\n",
    "\n",
    "        # Set learning rate by layer, if you like\n",
    "        if torch.cuda.device_count() > 1: # multi GPUs \n",
    "            plist = [\n",
    "                        {\n",
    "                            \"params\": model.module.stem.parameters(),\n",
    "                            \"lr\": self.lr * 0.1,\n",
    "                        },\n",
    "                        {\n",
    "                            \"params\": model.module.blocks[:16].parameters(),\n",
    "                            \"lr\": self.lr * 0.15,\n",
    "                        },\n",
    "                        {\n",
    "                            \"params\": model.module.blocks[16:].parameters(),\n",
    "                            \"lr\": self.lr * 0.2,\n",
    "                        },\n",
    "                        {\n",
    "                            \"params\": model.module.head[:6].parameters(),\n",
    "                            \"lr\": self.lr * 0.3,\n",
    "                        },    \n",
    "                        {\n",
    "                            \"params\": model.module.head[6].parameters(), \n",
    "                            \"lr\": self.lr,\n",
    "                        }\n",
    "            ]\n",
    "        else: # single GPU\n",
    "            pass\n",
    "\n",
    "        \n",
    "        #optimizer = RAdam(plist,weight_decay=1e-4, lr=self.lr)\n",
    "        optimizer = optim.Adam(plist,weight_decay=1e-4, lr=self.lr)\n",
    "\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.T_max, eta_min=self.lr/10)\n",
    "        \n",
    "        if use_cv: # if you choose cv, split the data into train and valid\n",
    "            train_sampler = SubsetRandomSampler(train_idx)\n",
    "            valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "            data_loader_train = torch.utils.data.DataLoader(self.train_data, batch_size=self.bs,\n",
    "                                                num_workers=self.num_workers,sampler=train_sampler, \n",
    "                                                            drop_last=True, shuffle=False)\n",
    "            data_loader_valid = torch.utils.data.DataLoader(self.train_data, batch_size=self.bs,\n",
    "                                                num_workers=self.num_workers,sampler=valid_sampler, \n",
    "                                                            drop_last=True, shuffle=False)\n",
    "            \n",
    "        # prepare train and valid data (e.g. train: 2015, valid: 2019)\n",
    "        elif self.use_valid:\n",
    "            data_loader_train = torch.utils.data.DataLoader(self.train_data, batch_size=self.bs,\n",
    "                                                num_workers=self.num_workers, drop_last=True,\n",
    "                                                            shuffle=True)\n",
    "            data_loader_valid = torch.utils.data.DataLoader(self.valid_data, batch_size=self.bs,\n",
    "                                                num_workers=self.num_workers, drop_last=True,\n",
    "                                                            shuffle=False)\n",
    "        else: # no validation\n",
    "            data_loader_train = torch.utils.data.DataLoader(self.train_data, batch_size=self.bs,\n",
    "                                                num_workers=self.num_workers, drop_last=True,\n",
    "                                                            shuffle=True)\n",
    "            \n",
    "        if self.early_stop: \n",
    "            early_stopping = EarlyStopping(patience=self.patience, verbose=True)\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            # unfreeze layers if you like\n",
    "            if epoch == self.n_freeze:            \n",
    "                if torch.cuda.device_count() > 1: # multi GPUs\n",
    "                    for param in model.module.parameters():\n",
    "                        param.requires_grad = True\n",
    "                else: # single GPU\n",
    "                    for param in model.parameters():\n",
    "                        param.requires_grad = True         \n",
    "\n",
    "            print('Epoch {}/{}'.format(epoch, self.n_epochs - 1))\n",
    "            print('-' * 10)\n",
    "            scheduler.step()\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            kappa = 0\n",
    "            steps = 0\n",
    "            with tqdm(data_loader_train, total=int(len(data_loader_train))) as pbar:\n",
    "                for bi, d in enumerate(pbar):\n",
    "                    inputs = d[\"image\"]\n",
    "                    labels = d[\"labels\"].view(-1, 1)\n",
    "                    inputs = inputs.to(self.device, dtype=torch.float)\n",
    "                    labels = labels.to(self.device, dtype=torch.long).view(-1)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    with torch.set_grad_enabled(True):\n",
    "                        outputs = model(inputs)\n",
    "                        loss =  self.criterion(outputs, labels)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    running_loss += loss.mean().item() \n",
    "                    \n",
    "                    y_hat = torch.Tensor.cpu(outputs).detach()\n",
    "                    y_hat = y_hat[:,:-1]\n",
    "                    # calc softmax for submission\n",
    "                    y_hat = F.softmax(y_hat, dim=-1)\n",
    "                    _,y_hat = torch.max(y_hat,1)\n",
    "                    y_hat = y_hat.float()\n",
    "                    y = torch.Tensor.cpu(labels.view(-1))\n",
    "                    kappa += quadratic_kappa(y_hat, y, self.coef).mean().item()\n",
    "                    steps += 1\n",
    "                    pbar.set_postfix(OrderedDict(loss = running_loss / steps,\n",
    "                                                 kappa_score = kappa / steps))\n",
    "                    \n",
    "            epoch_loss = running_loss / steps\n",
    "            print('Training Loss: {:.4f}'.format(epoch_loss))\n",
    "            # calculate kappa score only to monitor training\n",
    "            kappa = kappa / steps\n",
    "            print('Training Kappa: {:.4f}'.format(kappa))\n",
    "            \n",
    "            if self.use_valid or use_cv:\n",
    "                model.eval()\n",
    "                running_loss = 0.0\n",
    "                kappa = 0\n",
    "                steps = 0\n",
    "\n",
    "                true = np.zeros((len(self.valid_data), 1)) if not use_cv else np.zeros((len(valid_sampler), 1))\n",
    "                preds = np.zeros((len(self.valid_data), 1)) if not use_cv else np.zeros((len(valid_sampler), 1))\n",
    "\n",
    "                with tqdm(data_loader_valid , total=int(len(data_loader_valid))) as pbar:\n",
    "                    for step, batch in enumerate(pbar):\n",
    "                        inputs = batch[\"image\"]\n",
    "                        labels = batch[\"labels\"].view(-1, 1)\n",
    "                        inputs = inputs.to(self.device, dtype=torch.float)\n",
    "                        labels = labels.to(self.device, dtype=torch.long).view(-1)\n",
    "                    \n",
    "                        with torch.no_grad():\n",
    "                            outputs = model(inputs)\n",
    "                            loss =  self.criterion(outputs, labels)\n",
    "\n",
    "                        running_loss += loss.mean().item()     \n",
    "                        \n",
    "                        y_hat = torch.Tensor.cpu(outputs).detach()\n",
    "                        y_hat = y_hat[:,:-1]\n",
    "                        # calc softmax for submission\n",
    "                        y_hat = F.softmax(y_hat, dim=-1)\n",
    "                        _,y_hat = torch.max(y_hat,1)\n",
    "                        y_hat = y_hat.float()\n",
    "                        y = torch.Tensor.cpu(labels.view(-1))\n",
    "                        true[step * self.bs:(step + 1) * self.bs] += labels.detach().cpu().squeeze().numpy().ravel().reshape(-1, 1)\n",
    "                        preds[step * self.bs:(step + 1) * self.bs] += y_hat.numpy().ravel().reshape(-1, 1)\n",
    "                        kappa += quadratic_kappa(y_hat, y, self.coef).mean().item()\n",
    "                        steps += 1\n",
    "                        pbar.set_postfix(OrderedDict(loss = running_loss / steps,\n",
    "                                                     kappa_score = kappa / steps))\n",
    "                        \n",
    "                epoch_loss = running_loss / steps\n",
    "                print('Validation Loss: {:.4f}'.format(epoch_loss))\n",
    "                # calculate kappa score only to monitor training\n",
    "                kappa = kappa / steps\n",
    "                print('Validation Kappa: {:.4f}'.format(kappa))\n",
    "                # plot Confusion Matrix\n",
    "                plot_cmx(true.astype(int),preds.astype(int))\n",
    "                \n",
    "                # early stopping\n",
    "                if self.early_stop:\n",
    "                    eval_loss = epoch_loss\n",
    "                    early_stopping(eval_loss, model)\n",
    "                    if early_stopping.early_stop:\n",
    "                        print(\"Early stopping\")\n",
    "                        if use_cv:\n",
    "                            return early_stopping.val_loss_min\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "        \n",
    "        # save model except when cv\n",
    "        if not use_cv:\n",
    "            torch.save(model.state_dict(), self.save_path)\n",
    "\n",
    "        del(model, data_loader_train)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() \n",
    "        \n",
    "    def cv(self): # cross validation\n",
    "        num_train = len(self.train_data)\n",
    "        indices = list(range(num_train))\n",
    "        kf = KFold(n_splits=self.n_folds, random_state=1337, shuffle=True)\n",
    "\n",
    "        train_idx = []\n",
    "        valid_idx = []\n",
    "\n",
    "        for t, v in kf.split(indices):\n",
    "            train_idx.append(t)\n",
    "            valid_idx.append(v)\n",
    "\n",
    "        # Training\n",
    "        scores = np.zeros(self.n_folds)\n",
    "        for fold in np.arange(self.n_folds):\n",
    "            print('Fold:',fold)\n",
    "            scores[fold] = self.fit(use_cv=True,train_idx=train_idx[fold],valid_idx=valid_idx[fold])\n",
    "        print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#params\n",
    "params = {\n",
    "    \"lr\" : 1e-4,\n",
    "    \"batch_size\" : 64,\n",
    "    \"n_epochs\" : 20,\n",
    "    \"n_freeze\" : 1,\n",
    "    \"num_workers\" : 8,\n",
    "    \"coef\" : [0.5, 1.5, 2.5, 3.5],\n",
    "    \"criterion\" :  MultiTaskLoss(gamma=2., alpha=class_weights, second_loss=kappa_loss, second_mult=0.5),\n",
    "    \"load_state\" : False,\n",
    "    \"load_path\" : None, \n",
    "    'save_path' : None,\n",
    "    \"device\" : device,\n",
    "    \"n_folds\" : 5,\n",
    "    \"early_stop\" : False,\n",
    "    \"patience\" : 3,\n",
    "    \"T_max\" : 7,\n",
    "}\n",
    "\n",
    "#training for the lazy, like me\n",
    "\n",
    "# train:old & valid:new\n",
    "if 1:\n",
    "    params['n_freeze'] = 1\n",
    "    params['n_epochs'] = 40\n",
    "    params['early_stop'] = True\n",
    "    params['patience'] = 10\n",
    "    params['save_path'] =  \"ENb4_ori_cls_old.bin\"\n",
    "    Mytrain = train(params)\n",
    "    Mytrain.get_train(RetinopathyDataset(csv_file='../input/diabetic-retinopathy-resized/balanced_trainLabels.csv', \n",
    "                       transform = transform,\n",
    "                       datatype='train_old'))\n",
    "    Mytrain.get_valid(RetinopathyDataset(csv_file=\"../input/aptos2019-blindness-detection/train.csv\", \n",
    "                       transform = transform,\n",
    "                       datatype='train'))\n",
    "    Mytrain.fit()\n",
    "    !mv checkpoint.pt ENb4_ori_cls.pt\n",
    "    \n",
    "# train:new\n",
    "if 0:\n",
    "    params['n_freeze'] = 1\n",
    "    params['n_epochs'] = 6\n",
    "    params['lr'] = 1e-4\n",
    "    params['T_max'] = 6\n",
    "    params['load_state'] = True \n",
    "    params['load_path'] =  \"ENb4_ori_cls.pt\"\n",
    "    params['save_path'] =  \"ENb4_ori_cls_new.bin\"        \n",
    "    Mytrain = train(params)\n",
    "    Mytrain.get_train(RetinopathyDataset(csv_file=\"../input/aptos2019-blindness-detection/train.csv\", \n",
    "                       transform = transform,\n",
    "                       datatype='train'))\n",
    "    Mytrain.fit()\n",
    "    \n",
    "# cv: new\n",
    "if 0:\n",
    "    params['n_freeze'] = 1\n",
    "    params['n_epochs'] = 10\n",
    "    params['load_state'] = True\n",
    "    params['load_path'] =  \"ENb4_ori_cls.pt\"\n",
    "    Mytrain = train(params)\n",
    "    Mytrain.get_train(RetinopathyDataset(csv_file=\"../input/aptos2019-blindness-detection/train.csv\", \n",
    "                       transform = transform,\n",
    "                       datatype='train'))\n",
    "    Mytrain.cv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = \"ENb4_ori_cls_new.bin\"\n",
    "\n",
    "model = load_EfficientNet()\n",
    "# load params\n",
    "model_state = torch.load(load_path)\n",
    "# A basic remapping is required\n",
    "mapping = {\n",
    "    k: v for k, v in zip(model_state.keys(), model.state_dict().keys())\n",
    "}\n",
    "mapped_model_state = OrderedDict([\n",
    "    (mapping[k], v) for k, v in model_state.items()\n",
    "])\n",
    "model.load_state_dict(mapped_model_state, strict=False)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform = transform):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name =  os.path.join('../input/aptos2019-blindness-detection/test_images',\n",
    "                                 self.data.loc[idx, 'id_code'] + '.png')\n",
    "        img = Image.open(img_name)\n",
    "        # image preprocessing\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TestDataset(csv_file='../input/aptos2019-blindness-detection/sample_submission.csv',\n",
    "                           transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32,num_workers=8,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT = False\n",
    "if PLOT:\n",
    "    batch = next(iter(test_loader))\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Test Images\")\n",
    "    _ = plt.imshow( \n",
    "        vutils.make_grid(batch[:16], padding=2, normalize=True).cpu().numpy().transpose((1, 2, 0))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(output):\n",
    "    # calc softmax for submission\n",
    "    pred = F.softmax(output, dim=-1)\n",
    "    _,pred = torch.max(pred,1)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bM6q_rVj2hTR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_bs = 32\n",
    "test_preds = np.zeros((len(test_dataset), 1))\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_bs, shuffle=False, num_workers=8)\n",
    "\n",
    "for i, x_batch in tqdm(enumerate(tqdm(test_data_loader))):\n",
    "    x_batch = x_batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        output1 = model(x_batch).detach().cpu()\n",
    "        reg1 = output1[:,-1]\n",
    "        cls1 = test_prediction(output1[:,:-1]).float()\n",
    "        #Horizontal Flip\n",
    "        output2 = model(x_batch.flip(1)).detach().cpu()\n",
    "        reg2 = output2[:,-1]\n",
    "        cls2 = test_prediction(output2[:,:-1]).float()\n",
    "        #Vertical Flip\n",
    "        output3 = model(x_batch.flip(2)).detach().cpu()\n",
    "        reg3 = output3[:,-1]\n",
    "        cls3 = test_prediction(output3[:,:-1]).float()\n",
    "        \n",
    "        pred_reg = (reg1 + reg2 + reg3) / 3.\n",
    "        pred = (cls1 + cls2 + cls3 + pred_reg) / 4.\n",
    "        \n",
    "    test_preds[i * test_bs:(i + 1) * test_bs] += pred.squeeze().numpy().ravel().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model \n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bbGSxlHC2jpq"
   },
   "outputs": [],
   "source": [
    "coef = [0.5, 1.5, 2.5, 3.5]\n",
    "\n",
    "for i, pred in enumerate(test_preds):\n",
    "    if pred < coef[0]:\n",
    "        test_preds[i] = 0\n",
    "    elif pred >= coef[0] and pred < coef[1]:\n",
    "        test_preds[i] = 1\n",
    "    elif pred >= coef[1] and pred < coef[2]:\n",
    "        test_preds[i] = 2\n",
    "    elif pred >= coef[2] and pred < coef[3]:\n",
    "        test_preds[i] = 3\n",
    "    else:\n",
    "        test_preds[i] = 4\n",
    "\n",
    "sample = pd.read_csv(\"../input/aptos2019-blindness-detection/sample_submission.csv\")\n",
    "sample.diagnosis = test_preds.astype(int)\n",
    "sample.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[\"diagnosis\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "APTOS.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
